# Raspagem de dados
## O que vamos Aprender
Hoje vamos aprender o que √© raspagem de dados e o que podemos fazer utilizando esta t√©cnica. Vamos ver tamb√©m como fazer requisi√ß√µes web, analisando suas respostas e extraindo dados. Al√©m disso, tamb√©m vamos ver t√©cnicas para evitar problemas com essa pr√°tica.

## Vamos aprender:
- Realizar requisi√ß√µes web;
- Analizar conte√∫dos HTML afim de extrair dados;
- Aplicar t√©cnicas de raspagem de dados para evitar problemas como bloqueio de acesso;
- Armazenar os dados obitidos em um banco de dados

## Por que isso √© importante?

Imagine que voc√™ queira fazer compara√ß√µes de pre√ßos e informa√ß√µes, ou talvez descobrir informa√ß√µes sobre algum assunto. Todo o dado a respeito, porem, n√£o esta estruturado, sendo exibido somente uma pagina web. Uma pagina web pode ser util para humanos, mas certamente n√£o util para fazermos analises estruturadas.

A raspagem de dados tem sido muito √∫til em trabalhos jornal√≠sticos, fornecendo dados para sustentarem mat√©rias, mas tamb√©m pode ser √∫til para comparar pre√ßos de produtos com a concorr√™ncia, automatiza√ß√£o de processos ma√ßantes como buscar artigos cient√≠ficos em bases acad√™micas, recupera√ß√£o de documentos em sites jur√≠dicos, analisar perfis de redes sociais, recuperar dados p√∫blicos do governo e muitos outros lugares.
Caso tenha repetido um processo mais de duas vezes, automatize-o.
Dado a infinidade de usos provavelmente, em sua carreira como pessoa desenvolvedora, voc√™ ir√° se deparar em algum momento com a necessidade de escrever um programa que realize a extra√ß√£o de dados para algum prop√≥sito.

## Introdu√ß√£o
Raspagem de dados √© uma t√©cnica computacional para extrair dados provenientes de um servi√ßo ou aplica√ß√£o, estruturando-os em banco de dados, planilhas, ou outros formatos. Consiste em extrair dados de sites e transport√°-los para um formato mais simples e male√°vel para que possam ser analisados e cruzados com mais facilidade.
Alguns passos aplicados nesta t√©cnica s√£o a realiza√ß√£o de requisi√ß√µes, an√°lise das resposta e extra√ß√£o dos dados, navega√ß√£o do conte√∫do, limpeza e armazenamento dos dados.
Vamos passo a passo, aprendendo como fazer e prevenindo erros que podem acontecer.

## Requisi√ß√£o web

A comunica√ß√£o com servidores HTTP e HTTPS se d√° atrav√©s de requisi√ß√µes, em que utilizamos o verbo para indicar que tipo de a√ß√£o deve ser tomada para um dado recurso. Devemos informar na requisi√ß√£o em qual recurso estamos atuando e no cabe√ßalho passamos algumas informa√ß√µes que podem ser interessantes como o tipo de resposta aceita ou o tipo de conte√∫do sendo enviado.
O Python possui um pacote para lidar com o protocolo HTTP, por√©m este n√£o √© t√£o amig√°vel para seres humanos. Por isso vamos utilizar a biblioteca requests , que possui uma interface bem mais amig√°vel. Ela pode ser instalada utilizando o comando abaixo, mas lembre-se de criar um ambiente virtual antes de instalar bibliotecas. Para recordar como criar um ambiente isolado, acesse o conte√∫do .

 python3 -m pip install requests

 Vamos ver alguns exemplos abaixo de como utilizar a biblioteca requests :


import requests


# Requisi√ß√£o do tipo GET
response = requests.get("https://www.betrybe.com/")
print(response.status_code)  # c√≥digo de status
print(response.headers["Content-Type"])  # conte√∫do no formato html

# Conte√∫do recebido da requisi√ß√£o
print(response.text)

# Bytes recebidos como resposta
print(response.content)

# Requisi√ß√£o do tipo post
response = requests.post("http://httpbin.org/post", data="some content")
print(response.text)

# Requisi√ß√£o enviando cabe√ßalho (header)
response = requests.get("http://httpbin.org/get", headers={"Accept": "application/json"})
print(response.text)

# Requisi√ß√£o a recurso bin√°rio
response = requests.get("http://httpbin.org/image/png")
print(response.content)

# Recurso JSON
response = requests.get("http://httpbin.org/get")
# Equivalente ao json.loads(response.content)
print(response.json())

# Podemos tamb√©m pedir que a resposta lance uma exce√ß√£o caso o status n√£o seja OK
response = requests.get("http://httpbin.org/status/404")
response.raise_for_status()


## Alguns Problemas

## Rate Limit

Muitas vezes a p√°gina de onde estamos removendo o conte√∫do possui uma limita√ß√£o de quantas requisi√ß√µes podemos enviar em um curto per√≠odo de tempo, evitando assim ataques de nega√ß√£o de servi√ßo.
Isto pode nos levar a ficarmos bloqueados por um curto per√≠odo de tempo ou at√© mesmo banidos de acessar um recurso.
Que tal vermos um exemplo?

import requests


# √Ä partir da d√©cima requisi√ß√£o somos bloqueados de acessar o recurso
# C√≥digo de status 429: Too Many Requests
for _ in range(15):
    response = requests.get("https://www.cloudflare.com/rate-limit-test/")
    print(response.status_code)

Neste exemplo, ap√≥s a d√©cima requisi√ß√£o, o servidor come√ßa a nos retornar respostas com o c√≥digo de status 429 , "Too Many Requests". Isto significa que estamos utilizando uma taxa de solicita√ß√£o maior que a suportada. Ele permanecer√° assim por algum tempo (1 minuto).

Uma boa pr√°tica √© sempre colocarmos um uma pausa entre as requisi√ß√µes, ou lote delas, mesmo que o website, onde a informa√ß√£o est√°, n√£o fa√ßa o bloqueio, assim evitamos tirar o site do ar.


import requests
import time


# Coloca uma pausa de 6 segundos a cada requisi√ß√£o
# Obs: este site de exemplo tem um rate limit de 10 requisi√ß√µes por minuto
for _ in range(15):
    response = requests.get("https://www.cloudflare.com/rate-limit-test/")
    print(response)
    time.sleep(6)


## Timeout
√Ås vezes pedimos um recurso ao servidor, mas caso o nosso tr√°fego de rede esteja lento ou tenha um problema interno do servidor, nossa resposta pode demorar ou at√© mesmo ficar travada indefinidamente.
Como garantir, ap√≥s um tempo, que o recurso seja retornado? ü§î

import requests

# Por 10 segundos n√£o temos certeza se a requisi√ß√£o ir√° retornar
response = requests.get("https://httpbin.org/delay/10")
print(response)

Podemos definir um tempo limite (timeout) para que, ap√≥s este tempo, possamos tomar alguma atitude como por exemplo, realizar uma nova tentativa.
Este tempo limite normalmente √© definido atrav√©s de experimenta√ß√µes e an√°lise do tempo de resposta normal de uma requisi√ß√£o.

import requests


try:
    # recurso demora muito a responder
    response = requests.get("http://httpbin.org/delay/10", timeout=2)
except requests.ReadTimeout:
    # vamos fazer uma nova requisi√ß√£o
    response = requests.get("http://httpbin.org/delay/1", timeout=2)
finally:
    print(response.status_code)


No exemplo acima, para efeitos did√°ticos, modificamos a URI do recurso, diminuindo o delay de resposta da requisi√ß√£o, atrav√©s do timeout , por√©m normalmente este valor n√£o muda.

## Analizando respostas
üóí Para realizar a extra√ß√£o de dados de um conte√∫do web vamos utilizar uma biblioteca chamada parsel . Ela pode ser instalada com o comando o comando abaixo:

 python3 -m pip install parsel

Como j√° aprendemos a realizar requisi√ß√µes HTTP e buscar seu conte√∫do, vamos agora analisar este conte√∫do e extrair informa√ß√µes.
Para ajudar na did√°tica, vamos utilizar o site http://books.toscrape.com/ . Ele √© um site pr√≥prio para o treinamento de raspagem de dados.
Para come√ßar, vamos acessar o site e ver o seu conte√∫do.

Em c√≥digo, vamos baixar o conte√∫do da p√°gina e criar um seletor, que ser√° utilizado para realizarmos as extra√ß√µes. Vamos criar o arquivo .py para buscarmos as informa√ß√µes:
exemplo_scrape.py

from parsel import Selector
import requests


response = requests.get("http://books.toscrape.com/")
selector = Selector(text=response.text)
print(selector)


Ok , que tal extrairmos todos os livros desta primeira p√°gina e buscar seus t√≠tulos e pre√ßos?
Para conseguirmos extrair essas informa√ß√µes precisamos, primeiro, inspecionar cada um dos elementos, buscando algo que os identifique de forma √∫nica na p√°gina.

    exemplo_scrape.py

# ...


# response = requests.get("http://books.toscrape.com/")
# selector = Selector(text=response.text)

# O t√≠tulo est√° no atributo title em um elemento √¢ncora (<a>)
# Dentro de um h3 em elementos que possuem classe product_pod
titles = selector.css(".product_pod h3 a::attr(title)").getall()
# Estamos utilizando a::attr(title) para capturar somente o valor contido no texto do seletor

# Os pre√ßos est√£o no texto de uma classe price_color
# Que se encontra dentro da classe .product_price
prices = selector.css(".product_price .price_color::text").getall()

# Combinando tudo podemos buscar os produtos
# em em seguida buscar os valores individualmente
for product in selector.css(".product_pod"):
    title = product.css("h3 a::attr(title)").get()
    price = product.css(".price_color::text").get()
    print(title, price)


O seletor principal que cont√©m todo o conte√∫do da p√°gina pode ser utilizado em uma busca para encontrar seletores mais espec√≠ficos. Podemos fazer isto utilizando a fun√ß√£o css . Ela recebe um seletor CSS como par√¢metro, embora podemos passar um tipo especial de seletor quando queremos algum valor bem espec√≠fico como o texto ou um atributo.
Ap√≥s definir o seletor, podemos utilizar a fun√ß√£o get para buscar o primeiro seletor/valor que satisfa√ßa aquela busca. A fun√ß√£o getall √© similar, por√©m tr√°s todos os valores que satisfa√ßam aquele seletor.
Assim como temos a fun√ß√£o css que faz a busca por seletores CSS, temos tamb√©m a fun√ß√£o xpath que faz a busca com base em XPath.
üí° Existem sites que podem ajudar com seletores css ou xpath .

## recursos Paginados

Tudo certo, temos 20 livros, mas sabemos que na verdade este site possui 1000 livros. que tal coletamos todos

Navegando um pouco por entre as p√°ginas, percebemos que cada p√°gina possui uma refer√™ncia para a pr√≥xima. Mas, se a URL √© sequencial, por que n√£o simplesmente jogamos valores at√© a p√°gina avisar que o recurso n√£o foi encontrado?
Acontece que podemos evitar fazer requisi√ß√µes desnecess√°rias, j√° que a p√°gina nos informa a pr√≥xima.
O que precisamos fazer √© criar um seletor com a p√°gina, extrair os dados, buscar a nova p√°gina e repetir todo o processo, at√© termos navegados em todas as p√°ginas.
At√© a parte da extra√ß√£o n√≥s j√° fizemos, vamos ent√£o dar uma olhada em como buscar a pr√≥xima p√°gina.


 exemplo_scrape.py

# ...
# for product in selector.css(".product_pod"):
#     title = product.css("h3 a::attr(title)").get()
#     price = product.css(".price_color::text").get()
#     print(title, price)

# Existe uma classe next, que podemos recuperar a url atrav√©s do seu elemento √¢ncora
next_page_url = selector.css(".next a::attr(href)").get()
print(next_page_url)

Agora que sabemos como recuperar a pr√≥xima p√°gina, podemos iniciar na primeira p√°gina e continuar buscando livros enquanto uma nova p√°gina for encontrada. Cada vez que encontrarmos uma p√°gina, extra√≠mos seus dados e continuamos at√© acabarem as p√°ginas. Ent√£o vamos alterar tudo que t√≠nhamos escrito no arquivo exemplo_scrape.py para o c√≥digo abaixo:
exemplo_scrape.py

from parsel import Selector
import requests


# Define a primeira p√°gina como pr√≥xima a ter seu conte√∫do recuperado
URL_BASE = "http://books.toscrape.com/catalogue/"
next_page_url = 'page-1.html'
while next_page_url:
    # Busca o conte√∫do da pr√≥xima p√°gina
    response = requests.get(URL_BASE + next_page_url)
    selector = Selector(text=response.text)
    # Imprime os produtos de uma determinada p√°gina
    for product in selector.css(".product_pod"):
        title = product.css("h3 a::attr(title)").get()
        price = product.css(".price_color::text").get()
        print(title, price)
    # Descobre qual √© a pr√≥xima p√°gina
    next_page_url = selector.css(".next a::attr(href)").get()

Nossa, quantos livros! üìö